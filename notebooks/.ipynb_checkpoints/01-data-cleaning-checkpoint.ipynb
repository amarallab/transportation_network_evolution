{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook prepare raw data into network data for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating US air networks from source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def strip_accents(text):\n",
    "    \"\"\"\n",
    "    Strip accents from input String.\n",
    "\n",
    "    :param text: The input string.\n",
    "    :type text: String.\n",
    "\n",
    "    :returns: The processed String.\n",
    "    :rtype: String.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except (TypeError, NameError): # unicode is a default on python 3 \n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def text_to_id(text):\n",
    "    \"\"\"\n",
    "    Convert input text to id.\n",
    "\n",
    "    :param text: The input string.\n",
    "    :type text: String.\n",
    "\n",
    "    :returns: The processed String.\n",
    "    :rtype: String.\n",
    "    \"\"\"\n",
    "    text = strip_accents(text.lower())\n",
    "    text = re.sub(r\"\\d\", \"\", text) \n",
    "    text=re.sub(r\"^\\s+\", \"\", text) \n",
    "    text=re.sub(r\"\\s+$\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\",\"_\", text, flags = re.I)\n",
    "    #text = re.sub('[ ]+', '_', text)\n",
    "    text = re.sub('[^a-zA-Z_-]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=np.arange(0,29,1)+1\n",
    "dfs=[]\n",
    "for i in n:    \n",
    "    df=pd.read_csv('/home/weihua/Research/Link_Dynamics/data/us_air_raw_data/770841524_T_T100D_MARKET_ALL_CARRIER-%s.csv' %i)\n",
    "    df=df[['YEAR','MONTH','ORIGIN_CITY_NAME','DEST_CITY_NAME','PASSENGERS']]\n",
    "    df=df.rename(index=str, columns={\"ORIGIN_CITY_NAME\": \"source\", \"DEST_CITY_NAME\": \"target\",'PASSENGERS':'weight'})\n",
    "    df['source']=df.apply(lambda row: text_to_id(str(row.source)), axis=1)\n",
    "    df['target']=df.apply(lambda row: text_to_id(str(row.target)), axis=1)\n",
    "    df=df.groupby(['YEAR','MONTH','source','target']).count()\n",
    "    df=df.reset_index()\n",
    "    dfs.append(df[df.weight !=0 ])\n",
    "data=pd.concat(dfs, ignore_index=True)\n",
    "data=data.reset_index().drop(columns='index')\n",
    "data.set_index(['YEAR', 'MONTH'], inplace=True)\n",
    "data.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <th colspan=\"5\" halign=\"left\">1990</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTH</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>adak_island_ak</td>\n",
       "      <td>adak_island_ak</td>\n",
       "      <td>akron_oh</td>\n",
       "      <td>akron_oh</td>\n",
       "      <td>akron_oh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>anchorage_ak</td>\n",
       "      <td>shemya_ak</td>\n",
       "      <td>atlanta_ga</td>\n",
       "      <td>birmingham_al</td>\n",
       "      <td>chicago_il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "YEAR              1990                                                       \n",
       "MONTH                1               1           1              1           1\n",
       "source  adak_island_ak  adak_island_ak    akron_oh       akron_oh    akron_oh\n",
       "target    anchorage_ak       shemya_ak  atlanta_ga  birmingham_al  chicago_il\n",
       "weight               1               1           1              1           2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to_pickle('/home/weihua/Research/Link_Dynamics/data/networks/US_air_1990_2018.pkl')\n",
    "data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air = pd.read_pickle('../data/networks/US_air_1990_2018.pkl')\n",
    "df_air = df_air[df_air.source != df_air.target]\n",
    "year = list(df_air.index.get_level_values(0).unique())\n",
    "month = list(df_air.index.get_level_values(1).unique())\n",
    "graphs_air = []\n",
    "date_air = []\n",
    "for y in year:\n",
    "    for m in month:\n",
    "#        if y != 2018 or m != 12:\n",
    "        df = df_air.loc[y,m]\n",
    "        date_air.append(date(y,m,1))\n",
    "        G = nx.from_pandas_edgelist(df_air.loc[y,m], edge_attr=True)\n",
    "        graphs_air.append(G)\n",
    "graphs_air=graphs_air"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat brazil bus netwroks from source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_datasets(data):\n",
    "    allcities=pd.read_csv(\"../data/buses_list_of_cities.csv\",index_col=0,names=[\"CityUF\"],encoding=\"utf-8\")\n",
    "    allcities.head()    \n",
    "    allcities[\"CityUF\"]=allcities[\"CityUF\"].str.upper()\n",
    "    allcities[\"CityUF\"]=allcities[\"CityUF\"].str.strip()\n",
    "    allcities[\"CityUF\"]=allcities[\"CityUF\"].str.replace(\" , \",\", \")\n",
    "    allcities[\"CityUF\"]=allcities[\"CityUF\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    setallcities=set(list(allcities.CityUF.astype(str)))    \n",
    "    data=data[data['ORIGEM'].isin(list(setallcities))]\n",
    "    data=data[data['DESTINO'].isin(list(setallcities))]    \n",
    "    return data\n",
    "\n",
    "def bus_network(year=2010,month=12):\n",
    "    data=pd.read_csv(\"../data/cleaned_buses_data/{}.csv\".format(year),index_col=None)\n",
    "    if month is not False:\n",
    "        data=data[data.MES==month]\n",
    "    data=data[data.NUMEROLUGAROFERTADOIDA>0]\n",
    "    data=match_datasets(data)\n",
    "    data=data[[\"ORIGEM\",\"DESTINO\"]]\n",
    "    data=data.groupby(data.columns.tolist()).size().reset_index().rename(columns={0:'weight'})\n",
    "    data=data.rename(columns={'ORIGEM':'source','DESTINO':'target','NUMEROLUGAROFERTADOIDA':'weight'})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bus = pd.read_pickle('../data/networks/bus_2005_2014.pkl')\n",
    "df_bus = df_bus[df_bus.source != df_bus.target]\n",
    "year = list(df_bus.index.get_level_values(0).unique())\n",
    "month = list(df_bus.index.get_level_values(1).unique())\n",
    "graphs_bus = []\n",
    "date_bus = []\n",
    "for y in year:\n",
    "    for m in month:\n",
    "#        if y != 2018 or m != 12:\n",
    "        df = bus_network(y,m)\n",
    "        date_bus.append(date(y,m,1))\n",
    "        G = nx.from_pandas_edgelist(df, edge_attr=True)\n",
    "        graphs_bus.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
